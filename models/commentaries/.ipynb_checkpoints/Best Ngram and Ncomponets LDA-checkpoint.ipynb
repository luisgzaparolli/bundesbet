{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit, StratifiedKFold\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, precision_score, recall_score, plot_roc_curve, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('words.p', 'rb') as fp:\n",
    "    my_words = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ger_df(ngram,n_com):\n",
    "    df=pd.read_csv('../../src/data/comments.csv')\n",
    "    X = list(df['comm'])\n",
    "    dataprep = Pipeline([('count_vectorizer', CountVectorizer(ngram_range=ngram, min_df=10, stop_words=my_words))])\n",
    "    pipeline = Pipeline([\n",
    "        ('dataprep', dataprep),\n",
    "        ('topic_modelling', LatentDirichletAllocation(n_components=n_com, random_state=42,n_jobs=-1))])\n",
    "    pipeline.fit(X)\n",
    "    topic_values = pipeline.transform(X)\n",
    "    df['labels'] = topic_values.argmax(axis=1)\n",
    "    def who_wins(row):\n",
    "        result = 'Away' if row['home_goals_final'] < row['away_goals_final'] else 'Home' if row['home_goals_final'] >row['away_goals_final'] else 'Draw'\n",
    "        return result\n",
    "    df['result']= df.apply(lambda row: who_wins(row), axis=1)\n",
    "    df.drop(columns=['home_goals_final','away_goals_final'], inplace=True)\n",
    "    X=pd.get_dummies(df, columns=['labels'])\n",
    "    df_group=X.groupby(['id_game','team','result']).sum().reset_index()\n",
    "    df_group.drop(columns=['time'],inplace= True)\n",
    "    df_home=df_group.iloc[::2]\n",
    "    home_columns= ['labels_'+str(i)+'_home' for i in range(0,n_com)]\n",
    "    home_columns=['id_game','team_home','result']+home_columns\n",
    "    df_home.columns=home_columns\n",
    "    df_away=df_group.iloc[1::2]\n",
    "    away_columns= ['labels_'+str(i)+'_away' for i in range(0,n_com)]\n",
    "    away_columns=['id_game','team_away','result']+away_columns\n",
    "    df_away.columns=away_columns\n",
    "    df_result=pd.merge(left=df_home, right=df_away, on=['id_game','result'])\n",
    "    df_result=df_result.drop(columns=['id_game'])\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(lda_df):\n",
    "    def func(x):\n",
    "        \"\"\"\n",
    "        Applying a function that change values of a Draw Match to 0, when Home team won to 1 and Away team won to -1.\n",
    "        \"\"\"\n",
    "        if x == 'Draw':\n",
    "            return 0\n",
    "        elif x == 'Home':\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def apply_func(dataframe, column):\n",
    "        dataframe[column] = dataframe[column].apply(func)\n",
    "    apply_func(lda_df, 'result')\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    lda_df[['team_home','team_away']] = lda_df[['team_home','team_away']].apply(le.fit_transform)\n",
    "    X = lda_df.drop('result', axis=1)\n",
    "    y = lda_df['result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 42,\n",
    "                                                        stratify=y)\n",
    "    model = RandomForestClassifier(bootstrap = True, criterion = 'gini', max_depth = 2,\n",
    "                               max_features = 'auto', min_samples_leaf = 10, \n",
    "                               min_samples_split = 5, n_estimators = 4, random_state=42,n_jobs=-1)\n",
    "    pipeline = Pipeline(steps=[('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    pipeline.predict(X_test)\n",
    "    score=pipeline.score(X_test, y_test).round(3)\n",
    "    roc=roc_auc_score(y_test, pipeline.predict_proba(X_test), multi_class='ovr').round(3)\n",
    "    precision=precision_score(y_test, pipeline.predict(X_test), average='weighted').round(3)\n",
    "    return score, roc, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 1), N_com: 5, SCORE: 0.426,ROC_AUC: 0.482,  PRECISION: 0.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 1), N_com: 10, SCORE: 0.447,ROC_AUC: 0.645,  PRECISION: 0.342\n",
      "N_GRAM: (1, 1), N_com: 15, SCORE: 0.404,ROC_AUC: 0.513,  PRECISION: 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [01:27<05:51, 87.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 1), N_com: 20, SCORE: 0.34,ROC_AUC: 0.439,  PRECISION: 0.261\n",
      "N_GRAM: (1, 2), N_com: 5, SCORE: 0.277,ROC_AUC: 0.599,  PRECISION: 0.257\n",
      "N_GRAM: (1, 2), N_com: 10, SCORE: 0.362,ROC_AUC: 0.481,  PRECISION: 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 2), N_com: 15, SCORE: 0.426,ROC_AUC: 0.579,  PRECISION: 0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      " 40%|████      | 2/5 [02:55<04:23, 87.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 2), N_com: 20, SCORE: 0.426,ROC_AUC: 0.511,  PRECISION: 0.335\n",
      "N_GRAM: (1, 3), N_com: 5, SCORE: 0.489,ROC_AUC: 0.561,  PRECISION: 0.541\n",
      "N_GRAM: (1, 3), N_com: 10, SCORE: 0.319,ROC_AUC: 0.48,  PRECISION: 0.302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 3), N_com: 15, SCORE: 0.489,ROC_AUC: 0.483,  PRECISION: 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [04:34<03:02, 91.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 3), N_com: 20, SCORE: 0.447,ROC_AUC: 0.565,  PRECISION: 0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 4), N_com: 5, SCORE: 0.426,ROC_AUC: 0.445,  PRECISION: 0.335\n",
      "N_GRAM: (1, 4), N_com: 10, SCORE: 0.362,ROC_AUC: 0.387,  PRECISION: 0.297\n",
      "N_GRAM: (1, 4), N_com: 15, SCORE: 0.319,ROC_AUC: 0.392,  PRECISION: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [06:01<01:30, 90.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (1, 4), N_com: 20, SCORE: 0.383,ROC_AUC: 0.419,  PRECISION: 0.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (2, 2), N_com: 5, SCORE: 0.362,ROC_AUC: 0.523,  PRECISION: 0.286\n",
      "N_GRAM: (2, 2), N_com: 10, SCORE: 0.277,ROC_AUC: 0.441,  PRECISION: 0.243\n",
      "N_GRAM: (2, 2), N_com: 15, SCORE: 0.404,ROC_AUC: 0.503,  PRECISION: 0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [07:07<00:00, 85.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_GRAM: (2, 2), N_com: 20, SCORE: 0.34,ROC_AUC: 0.457,  PRECISION: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_grams = [(1,1),(1,2),(1,3),(1,4),(2,2)]\n",
    "components=[5,10,15,20]\n",
    "for n_gram in tqdm(n_grams):\n",
    "    for n_com in components:\n",
    "        score, roc, precision=get_results(ger_df(n_gram,n_com))\n",
    "        print(f'N_GRAM: {n_gram}, N_com: {n_com}, SCORE: {score},ROC_AUC: {roc},  PRECISION: {precision}')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
