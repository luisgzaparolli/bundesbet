{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Get name of players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_players(url):\n",
    "    html=requests.get(url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    table_player=soup.find('table', {'class':'wisbb_standardTable'})\n",
    "    players=table_player.find_all('a', {'class':'wisbb_fullPlayer'})\n",
    "    lst_player = [item.find('span').text.strip().split(',') for item in players]\n",
    "    lst_player=[item.strip().lower() for x in lst_player for item in x]\n",
    "    return lst_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links():\n",
    "    url='https://www.foxsports.com/soccer/players?competition=4&teamId=0&season=2019&position=0&page=1&country=0&grouping=0&weightclass=0'\n",
    "    html=requests.get(url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    last_page=int(soup.find('div', {'class':'wisbb_paginator'}).find_all('a')[-2].text.strip())\n",
    "    first_url='https://www.foxsports.com/soccer/players?competition=4&teamId=0&season=2019&position=0&page='\n",
    "    second_url='&country=0&grouping=0&weightclass=0'\n",
    "    links = [first_url + str(item) + second_url for item in range(1,last_page+1)]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=get_links()\n",
    "results=map(get_players,links)\n",
    "players_names=list(results)\n",
    "players_names=[item for x in players_names for item in x]\n",
    "my_words += players_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get name of referees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.soccerbase.com/referees/home.sd?comp_id=20'\n",
    "html=requests.get(url).content\n",
    "soup=BeautifulSoup(html)\n",
    "referees=soup.find_all('td', {'class':'first bull'})\n",
    "referees = [item.text.lower().split(' ') for item in referees]\n",
    "referees_names = [item for x in referees for item in x]\n",
    "my_words += referees_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get name of stadiums, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_teams():\n",
    "    url='https://www.foxsports.com/soccer/standings?competition=4'\n",
    "    html=requests.get(url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    table_teams=soup.find('table', {'class':'wisbb_standardTable'})\n",
    "    teams=table_teams.find_all('a', {'class':'wisbb_fullTeam'})\n",
    "    links=['https://www.foxsports.com'+item['href']+'-schedule' for item in teams]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stadium_cities_names(url):\n",
    "    html=requests.get(url).content\n",
    "    soup=BeautifulSoup(html)\n",
    "    stadiums = soup.find_all('span', {'class':'wisbb_main'})\n",
    "    stadiums = [item.text.lower().split(' ') for item in stadiums]\n",
    "    stadiums_name=list(set([item for x in stadiums for item in x]))\n",
    "    location=soup.find_all('span', {'class':'wisbb_secondary'})\n",
    "    location = [item.text.lower().split(',') for item in location]\n",
    "    location_name=list(set([item.strip() for x in location for item in x]))\n",
    "    return stadiums_name + location_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=get_links_teams()\n",
    "results=map(get_stadium_cities_names,links)\n",
    "stadiums=list(results)\n",
    "stadiums=[item for x in stadiums for item in x]\n",
    "my_words += stadiums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing My words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=nltk.corpus.names.words('male.txt')\n",
    "names=[item.lower() for item in names]\n",
    "my_words += names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../src/data/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list=['period','current','half','dusseldorf','monchengladbach','signals','adjej', 'antwi', 'bazee', 'benno', \n",
    "         'benteler', 'bruun', 'chang', 'da', 'dicka', 'dong', 'eliyau', 'gebre', 'guzman', 'hoon', 'ilay', 'juste','07','tsg', \n",
    "         'larsen', 'levent', 'munir', 'neuberger', 'sarenren', 'schwarzwald', 'selassie', 'spiel', 'st', 'stadion', 'veltins', 'won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_words=list(df['team'].unique())\n",
    "team_words=[item.split(' ') for item in team_words]\n",
    "team_words=[item.lower() for x in team_words for item in x]\n",
    "my_words += team_words\n",
    "my_words += my_list\n",
    "my_words += gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dataprep',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('count_vectorizer',\n",
       "                                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                                  decode_error='strict',\n",
       "                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                  encoding='utf-8',\n",
       "                                                  input='content',\n",
       "                                                  lowercase=True, max_df=1.0,\n",
       "                                                  max_features=None, min_df=10,\n",
       "                                                  ngram_range=(1, 4),\n",
       "                                                  preprocessor=None,\n",
       "                                                  stop_words=['abdullahi',\n",
       "                                                              'suleiman',\n",
       "                                                              'abraham',...\n",
       "                          verbose=False)),\n",
       "                ('topic_modelling',\n",
       "                 LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                                           evaluate_every=-1,\n",
       "                                           learning_decay=0.7,\n",
       "                                           learning_method='batch',\n",
       "                                           learning_offset=10.0,\n",
       "                                           max_doc_update_iter=100, max_iter=10,\n",
       "                                           mean_change_tol=0.001,\n",
       "                                           n_components=8, n_jobs=-1,\n",
       "                                           perp_tol=0.1, random_state=42,\n",
       "                                           topic_word_prior=None,\n",
       "                                           total_samples=1000000.0,\n",
       "                                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(df['comm'])\n",
    "dataprep = Pipeline([('count_vectorizer', CountVectorizer(ngram_range=(1,4), min_df=10, stop_words=my_words))])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('dataprep', dataprep),\n",
    "    ('topic_modelling', LatentDirichletAllocation(n_components=8, random_state=42,n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pyLDAvis.sklearn\\n \\npyLDAvis.enable_notebook()\\npanel = pyLDAvis.sklearn.prepare(pipeline.named_steps.topic_modelling, \\n                                 pipeline.named_steps.dataprep.transform(X), \\n                                 pipeline.named_steps.dataprep.named_steps.count_vectorizer, \\n                                 mds='PcoA')\\n\\npanel\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pyLDAvis.sklearn\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(pipeline.named_steps.topic_modelling, \n",
    "                                 pipeline.named_steps.dataprep.transform(X), \n",
    "                                 pipeline.named_steps.dataprep.named_steps.count_vectorizer, \n",
    "                                 mds='PcoA')\n",
    "\n",
    "panel\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic=pipeline.named_steps.topic_modelling.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "topic_values = pipeline.transform(X)\n",
    "df['labels'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['team'])\n",
    "df['team_labels']=le.transform(df['team'])\n",
    "def who_wins(row):\n",
    "    result = 'Away' if row['home_goals_final'] < row['away_goals_final'] else 'Home' if row['home_goals_final'] >row['away_goals_final'] else 'Draw'\n",
    "    return result\n",
    "df['result']= df.apply(lambda row: who_wins(row), axis=1)\n",
    "df.drop(columns=['home_goals_final','away_goals_final'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'id_game', 'time', 'team', 'comm', 'team_labels', 'result',\n",
       "       'labels_0', 'labels_1', 'labels_2', 'labels_3', 'labels_4', 'labels_5',\n",
       "       'labels_6', 'labels_7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.get_dummies(df, columns=['labels'])\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group=X.groupby(['id_game','team_labels','result'])[['labels_0', 'labels_1',\n",
    "       'labels_2', 'labels_3', 'labels_4', 'labels_5', 'labels_6', 'labels_7']].sum().reset_index()\n",
    "df_home=df_group.iloc[::2]\n",
    "df_away=df_group.iloc[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home.columns=['id_game', 'team_home', 'result', 'labels_0_home', 'labels_1_home', 'labels_2_home',\n",
    "       'labels_3_home', 'labels_4_home', 'labels_5_home', 'labels_6_home', 'labels_7_home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_away.columns=['id_game', 'team_away', 'result', 'labels_0_away', 'labels_1_away', 'labels_2_away',\n",
    "       'labels_3_away', 'labels_4_away', 'labels_5_away', 'labels_6_away', 'labels_7_away']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.merge(left=df_home, right=df_away, on=['id_game','result'])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
